{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b5d423-00d1-4ea5-a619-74f1debbdcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Iceberg Configurations:\n",
      "Catalog type: hadoop\n",
      "Warehouse location: s3a://local-datalake/warehouse\n",
      "S3A endpoint: http://minio-service:9000\n",
      "\n",
      "Creating namespace...\n",
      "Namespace created: local.demo\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark with Iceberg catalog configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg-MinIO-Demo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"s3a://local-datalake/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl.disable.cache\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"20\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"fakesecret\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"fakesecret\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-service:9000\")\n",
    "\n",
    "# Use AnonymousAWSCredentialsProvider for read operations\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "# Add ACL permissions\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.acl.default\", \"PublicReadWrite\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.multipart.size\", \"5242880\")\n",
    "\n",
    "# Print Spark configurations to verify they are set correctly\n",
    "print(\"Spark Iceberg Configurations:\")\n",
    "print(f\"Catalog type: {spark.conf.get('spark.sql.catalog.local.type')}\")\n",
    "print(f\"Warehouse location: {spark.conf.get('spark.sql.catalog.local.warehouse')}\")\n",
    "print(f\"S3A endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "\n",
    "# Create namespace for our tables\n",
    "print(\"\\nCreating namespace...\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.demo\")\n",
    "print(\"Namespace created: local.demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc44854-32e7-4ad6-9e5e-9da8aebfdd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e79c50-5827-4a03-a1a5-fdc540bf53a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Iceberg table: local.demo.users\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import datetime\n",
    "\n",
    "# Define schema for our data\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"signup_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "current_time = datetime.datetime.now()\n",
    "data = [\n",
    "    (\"user1\", \"Alice\", 34, current_time - datetime.timedelta(days=30)),\n",
    "    (\"user2\", \"Bob\", 45, current_time - datetime.timedelta(days=25)),\n",
    "    (\"user3\", \"Carol\", 27, current_time - datetime.timedelta(days=20))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Write DataFrame to Iceberg table\n",
    "table_name = \"local.demo.users\"\n",
    "df.writeTo(table_name) \\\n",
    "    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"Successfully created Iceberg table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709c7e17-da98-46f8-afff-bd68f43034c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('user_id', StringType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('signup_date', TimestampType(), True)])\n"
     ]
    }
   ],
   "source": [
    "schema = spark.table(table_name).schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae62ee65-d98e-4ae4-82fd-d4deea2c46ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+--------------------+\n",
      "|user_id| name|age|         signup_date|\n",
      "+-------+-----+---+--------------------+\n",
      "|  user1|Alice| 34|2025-02-20 18:28:...|\n",
      "|  user2|  Bob| 45|2025-02-25 18:28:...|\n",
      "|  user3|Carol| 27|2025-03-02 18:28:...|\n",
      "+-------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(table_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204a9e3c-4425-4885-b671-eb035927c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|     demo|    users|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN local.demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07583950-cab9-4064-925c-476049ec47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-03-22 18:28:...|5519466210033624142|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = spark.sql(f\"SELECT * FROM {table_name}.history\")\n",
    "history_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b725bd-87e9-4e1a-82d1-9f60f93114a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|    user_id|   string|   NULL|\n",
      "|       name|   string|   NULL|\n",
      "|        age|      int|   NULL|\n",
      "|signup_date|timestamp|   NULL|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE TABLE {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85223dba-6685-4c5b-9ddb-c74011363e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended new data to local.demo.users\n",
      "+-------+-----+---+--------------------+\n",
      "|user_id| name|age|         signup_date|\n",
      "+-------+-----+---+--------------------+\n",
      "|  user6|Frank| 42|2025-03-22 18:32:...|\n",
      "|  user6|Frank| 42|2025-03-22 18:31:...|\n",
      "|  user1|Alice| 34|2025-02-20 18:28:...|\n",
      "|  user7|Grace| 31|2025-03-22 18:32:...|\n",
      "|  user2|  Bob| 45|2025-02-25 18:28:...|\n",
      "|  user7|Grace| 31|2025-03-22 18:31:...|\n",
      "|  user3|Carol| 27|2025-03-02 18:28:...|\n",
      "+-------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new data to the table\n",
    "new_data = [\n",
    "    (\"user6\", \"Frank\", 42, datetime.datetime.now()),\n",
    "    (\"user7\", \"Grace\", 31, datetime.datetime.now())\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "# Append new rows to the table\n",
    "try:\n",
    "    new_df.writeTo(table_name).append()\n",
    "    print(f\"Successfully appended new data to {table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error appending data: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    updated_df = spark.table(table_name)\n",
    "    updated_df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading updated data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2d97a4-be1f-4b59-af2f-1b3132dad576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error adding column: [FIELDS_ALREADY_EXISTS] Cannot add column, because `email` already exists in \"STRUCT<user_id: STRING, name: STRING, age: INT, signup_date: TIMESTAMP, email: STRING>\".; line 1 pos 0;\n",
      "AddColumns [QualifiedColType(None,email,StringType,true,None,None,None)]\n",
      "+- ResolvedTable org.apache.iceberg.spark.SparkCatalog@2abc31bd, demo.users, local.demo.users, [user_id#435, name#436, age#437, signup_date#438, email#439]\n",
      "\n",
      "\n",
      "Updated table schema:\n",
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|    user_id|   string|   NULL|\n",
      "|       name|   string|   NULL|\n",
      "|        age|      int|   NULL|\n",
      "|signup_date|timestamp|   NULL|\n",
      "|      email|   string|   NULL|\n",
      "+-----------+---------+-------+\n",
      "\n",
      "\n",
      "Table data with new column:\n",
      "+-------+-----+---+--------------------+-----+\n",
      "|user_id| name|age|         signup_date|email|\n",
      "+-------+-----+---+--------------------+-----+\n",
      "|  user6|Frank| 42|2025-03-22 18:31:...| NULL|\n",
      "|  user6|Frank| 42|2025-03-22 18:32:...| NULL|\n",
      "|  user1|Alice| 34|2025-02-20 18:28:...| NULL|\n",
      "|  user7|Grace| 31|2025-03-22 18:31:...| NULL|\n",
      "|  user2|  Bob| 45|2025-02-25 18:28:...| NULL|\n",
      "|  user7|Grace| 31|2025-03-22 18:32:...| NULL|\n",
      "|  user3|Carol| 27|2025-03-02 18:28:...| NULL|\n",
      "+-------+-----+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to the schema\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN email STRING\")\n",
    "    print(f\"Successfully added 'email' column to {table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding column: {str(e)}\")\n",
    "\n",
    "# View the updated schema\n",
    "print(\"\\nUpdated table schema:\")\n",
    "try:\n",
    "    spark.sql(f\"DESCRIBE TABLE {table_name}\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error describing table: {str(e)}\")\n",
    "\n",
    "# View the updated data\n",
    "print(\"\\nTable data with new column:\")\n",
    "try:\n",
    "    evolved_df = spark.table(table_name)\n",
    "    evolved_df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading evolved data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f457936-1135-47a8-9628-37cbb6fc61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table history:\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-03-22 18:28:...|5519466210033624142|               NULL|               true|\n",
      "|2025-03-22 18:31:...|6069084531131085546|5519466210033624142|               true|\n",
      "|2025-03-22 18:32:...|3818681792534402167|6069084531131085546|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Time travel to snapshot: 6069084531131085546\n",
      "Table data at previous snapshot:\n",
      "+-------+-----+---+--------------------+\n",
      "|user_id| name|age|         signup_date|\n",
      "+-------+-----+---+--------------------+\n",
      "|  user1|Alice| 34|2025-02-20 18:28:...|\n",
      "|  user6|Frank| 42|2025-03-22 18:31:...|\n",
      "|  user2|  Bob| 45|2025-02-25 18:28:...|\n",
      "|  user7|Grace| 31|2025-03-22 18:31:...|\n",
      "|  user3|Carol| 27|2025-03-02 18:28:...|\n",
      "+-------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, view the table history to get snapshot IDs\n",
    "print(\"Table history:\")\n",
    "try:\n",
    "    history_df = spark.sql(f\"SELECT * FROM {table_name}.history\")\n",
    "    history_df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error getting history: {str(e)}\")\n",
    "\n",
    "# Get snapshot IDs dynamically\n",
    "try:\n",
    "    snapshots = history_df.select(\"snapshot_id\").collect()\n",
    "    if len(snapshots) >= 2:\n",
    "        # Get the second most recent snapshot (before our latest changes)\n",
    "        previous_snapshot_id = snapshots[1][0]\n",
    "        \n",
    "        print(f\"\\nTime travel to snapshot: {previous_snapshot_id}\")\n",
    "        \n",
    "        # Read table at the previous snapshot\n",
    "        previous_df = spark.read.format(\"iceberg\").option(\"snapshot-id\", previous_snapshot_id).load(table_name)\n",
    "        print(\"Table data at previous snapshot:\")\n",
    "        previous_df.show()\n",
    "    else:\n",
    "        print(\"Not enough snapshots available for time travel demonstration\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during time travel: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
